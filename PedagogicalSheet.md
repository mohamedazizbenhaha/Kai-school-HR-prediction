This **6-hour workshop** equips **Bac+3/Bac+4 DevOps and Data Engineering** students with hands-on skills to build, deploy, and automate a simple AI model pipeline, from data preprocessing to a production-ready API, using modern DevOps tools and practices.

---

## Objectives

- Master the end-to-end AI workflow by preprocessing data, training a model, and deploying it as a functional pipeline.
- Implement DevOps practices, including containerization and CI/CD, to automate testing, building, and deployment.
- Develop a production-ready RESTful API to serve model predictions reliably and scalably.
- Collaborate effectively using Git for version control and team-based development.
- Evaluate and optimize AI and DevOps workflows to ensure robust, efficient solutions.

---

## Prerequisites

- Basic proficiency in Python programming.
- Familiarity with supervised machine learning concepts (e.g., classification).
- Understanding of web services and RESTful APIs.
- Basic command-line skills for navigating environments.
- Exposure to Git for version control.

---

## Target Competencies

By the end of the workshop, learners will be able to:

- Preprocess and analyze a dataset for machine learning tasks.
- Train and evaluate a simple supervised learning model.
- Build and deploy a RESTful API using Flask.
- Containerize an application using Docker.
- Configure a CI/CD pipeline with Jenkins and GitHub.

---

## Workshop Structure

### 1. Data Exploration and Preprocessing (2 hours)
Learners explore and clean the **"Adult Income" dataset from Kaggle**, preparing it for model training.

### 2. Model Training and API Development (2 hours)
Learners train a classification model and develop a **Flask API** with a `/predict` endpoint.

### 3. Containerization and CI/CD (2 hours)
Learners containerize the API with **Docker** and configure a **Jenkins pipeline** for automated testing, building, and deployment.
